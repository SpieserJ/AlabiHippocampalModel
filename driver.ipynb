{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(5)\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from astropy.stats import circmean, circvar\n",
    "import _pickle as pickle\n",
    "import os\n",
    "\n",
    "from networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "num_pc = 1000 # number of PC\n",
    "input_dim = 720 # BVC input size (720 bc RPLidar spits out a 720-point array)\n",
    "timestep = 32 * 3\n",
    "max_dist = 12 # max distance of LiDAR\n",
    "tau_w = 10 # time constant for the window function\n",
    "PI = tf.constant(np.pi) \n",
    "rng = default_rng() # random number generator\n",
    "cmap = get_cmap('plasma')\n",
    "goal_r = {\"explore\":.1, \"exploit\":.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('hmap_x.pkl', 'rb') as f:\n",
    "        hmap_x = pickle.load(f)\n",
    "    with open('hmap_y.pkl', 'rb') as f:\n",
    "        hmap_y = pickle.load(f)\n",
    "    with open('hmap_z.pkl', 'rb') as f:\n",
    "        hmap_z = np.asarray(pickle.load(f))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Driver:\n",
    "    \"\"\"\n",
    "    The Driver class controls the robot, manages its sensory inputs, and coordinates the activation of neural network layers \n",
    "    (place cells and reward cells) to simulate navigation and learning in an environment.\n",
    "\n",
    "    Attributes:\n",
    "        max_speed (float): The maximum speed of the robot.\n",
    "        left_speed (float): The current speed of the left wheel.\n",
    "        right_speed (float): The current speed of the right wheel.\n",
    "        timestep (int): The timestep for each simulation step.\n",
    "        wheel_radius (float): The radius of the robot's wheels.\n",
    "        axle_length (float): The distance between the robot's wheels.\n",
    "        run_time (int): The total run time for the simulation in seconds.\n",
    "        num_steps (int): The number of simulation steps based on run time and timestep.\n",
    "        sensor_data_x (ndarray): Array to store x-coordinates of sensor data.\n",
    "        sensor_data_y (ndarray): Array to store y-coordinates of sensor data.\n",
    "        place_cell_activations (ndarray): Array to store activations of place cells over time.\n",
    "        head_direction_activations (ndarray): Array to store head direction cell activations over time.\n",
    "        goal_estimates (ndarray): Array to store estimates of the goal location over time.\n",
    "        current_step (int): The current step in the simulation.\n",
    "        robot (object): Placeholder for the robot instance.\n",
    "        keyboard (object): Placeholder for the keyboard instance.\n",
    "        compass (object): Placeholder for the compass sensor.\n",
    "        range_finder (object): Placeholder for the range finder sensor.\n",
    "        left_bumper (object): Placeholder for the left bumper sensor.\n",
    "        right_bumper (object): Placeholder for the right bumper sensor.\n",
    "        display (object): Placeholder for the display instance.\n",
    "        rotation_field (object): Placeholder for the rotation field of the robot.\n",
    "        left_motor (object): Placeholder for the left motor of the robot.\n",
    "        right_motor (object): Placeholder for the right motor of the robot.\n",
    "        left_position_sensor (object): Placeholder for the left wheel position sensor.\n",
    "        right_position_sensor (object): Placeholder for the right wheel position sensor.\n",
    "        pcn (PlaceCellLayer): Instance of the place cell network.\n",
    "        rcn (RewardCellLayer): Instance of the reward cell network.\n",
    "        boundary_data (Tensor): Tensor to store boundary data from sensors.\n",
    "        goal_location (list): The coordinates of the goal location.\n",
    "        expected_reward (float): The expected reward at the current state.\n",
    "        last_reward (float): The reward received in the previous step.\n",
    "        context (int): Index of the current context in the environment.\n",
    "        s (Tensor): Tensor representing the current state.\n",
    "        s_prev (Tensor): Tensor representing the previous state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_place_cells=1000, num_reward_cells=10, num_head_directions=8, run_time_hours=2, timestep=96):\n",
    "        \"\"\"\n",
    "        Initializes the Driver class with specified parameters and sets up the robot's sensors and neural networks.\n",
    "\n",
    "        Parameters:\n",
    "            num_place_cells (int): Number of place cells in the place cell network.\n",
    "            num_reward_cells (int): Number of reward cells in the reward cell network.\n",
    "            num_head_directions (int): Number of head direction cells.\n",
    "            run_time_hours (int): Total run time for the simulation in hours.\n",
    "            timestep (int): The time step duration for each simulation step.\n",
    "        \"\"\"\n",
    "        # Robot parameters\n",
    "        self.max_speed = 4\n",
    "        self.left_speed = self.max_speed\n",
    "        self.right_speed = self.max_speed\n",
    "        self.timestep = timestep\n",
    "        self.wheel_radius = 0.031\n",
    "        self.axle_length = 0.271756\n",
    "        self.run_time_minutes = run_time_hours * 60\n",
    "        self.num_steps = int(self.run_time * 60 // (2 * self.timestep / 1000))\n",
    "\n",
    "        # Sensor data storage\n",
    "        self.sensor_data_x = np.zeros(self.num_steps)\n",
    "        self.sensor_data_y = np.zeros(self.num_steps)\n",
    "        self.place_cell_activations = np.zeros((self.num_steps, num_place_cells))\n",
    "        self.head_direction_activations = np.zeros((self.num_steps, num_head_directions))\n",
    "        self.goal_estimates = np.zeros(self.num_steps)\n",
    "\n",
    "        # Initialize timestep\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Initialize hardware components and sensors. Some of these won't be used.\n",
    "        self.robot = None  # Placeholder for robot instance\n",
    "        self.keyboard = None\n",
    "        self.compass = None\n",
    "        self.range_finder = None\n",
    "        self.left_bumper = None\n",
    "        self.right_bumper = None\n",
    "        self.display = None\n",
    "        self.rotation_field = None\n",
    "        self.left_motor = None\n",
    "        self.right_motor = None\n",
    "        self.left_position_sensor = None\n",
    "        self.right_position_sensor = None\n",
    "\n",
    "        # Initialize neural network layers\n",
    "        self.pcn = self.load_pcn(num_place_cells, num_head_directions)\n",
    "        self.rcn = self.load_rcn(num_reward_cells, num_place_cells)\n",
    "\n",
    "        # Initialize boundaries\n",
    "        self.boundary_data = tf.Variable(tf.zeros((720, 1)))\n",
    "\n",
    "        # Initialize goal and context\n",
    "        self.goal_location = None\n",
    "        self.expected_reward = 0\n",
    "        self.last_reward = 0\n",
    "        self.context = None\n",
    "        self.s = tf.zeros_like(self.pcn.place_cell_activations)\n",
    "        self.s_prev = tf.zeros_like(self.pcn.place_cell_activations)\n",
    "\n",
    "    def load_pcn(self, num_place_cells, num_head_directions):\n",
    "        \"\"\"\n",
    "        Loads the place cell network from a file if available, or initializes a new one.\n",
    "\n",
    "        Parameters:\n",
    "            num_place_cells (int): Number of place cells in the place cell network.\n",
    "            num_head_directions (int): Number of head direction cells.\n",
    "\n",
    "        Returns:\n",
    "            PlaceCellLayer: The loaded or newly initialized place cell network.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open('place_cell_network.pkl', \"rb\") as f:\n",
    "                pcn = pickle.load(f)\n",
    "                pcn.reset_activations()\n",
    "                print(\"Loaded existing Place Cell Network.\")\n",
    "        except:\n",
    "            pcn = PlaceCellLayer(num_place_cells, 720, self.timestep, 12, num_head_directions)\n",
    "            print(\"Initialized new Place Cell Network.\")\n",
    "        return pcn\n",
    "\n",
    "    def load_rcn(self, num_reward_cells, num_place_cells):\n",
    "        \"\"\"\n",
    "        Loads the reward cell network from a file if available, or initializes a new one.\n",
    "\n",
    "        Parameters:\n",
    "            num_reward_cells (int): Number of reward cells in the reward cell network.\n",
    "            num_place_cells (int): Number of place cells in the place cell network.\n",
    "\n",
    "        Returns:\n",
    "            RewardCellLayer: The loaded or newly initialized reward cell network.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open('reward_cell_network.pkl', 'rb') as f:\n",
    "                rcn = pickle.load(f)\n",
    "                print(\"Loaded existing Reward Cell Network.\")\n",
    "        except:\n",
    "            rcn = RewardCellLayer(num_reward_cells, num_place_cells, 3)\n",
    "            print(\"Initialized new Reward Cell Network.\")\n",
    "        return rcn\n",
    "\n",
    "    def startup(self, context, mode, randomize=False):\n",
    "        \"\"\"\n",
    "        Starts up the driver by initializing hardware, resetting the robot position, and setting the goal location.\n",
    "\n",
    "        Parameters:\n",
    "            context (int): The context or scenario index in the environment.\n",
    "            mode (str): The mode of operation (e.g., \"learn\", \"explore\").\n",
    "            randomize (bool): Whether to randomize the robot's starting position.\n",
    "        \"\"\"\n",
    "        self.context = context\n",
    "        self.mode = mode\n",
    "        self.initialize_hardware()\n",
    "        self.reset_robot_position(randomize)\n",
    "        self.set_goal_location(context)\n",
    "        self.sense()\n",
    "        self.compute()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
